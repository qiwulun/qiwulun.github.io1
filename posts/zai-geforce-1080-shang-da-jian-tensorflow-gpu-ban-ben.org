#+BEGIN_COMMENT
.. title: 在 Geforce 1080 上搭建 TensorFlow GPU 版本
.. slug: zai-geforce-1080-shang-da-jian-tensorflow-gpu-ban-ben
.. date: 2016-11-10 17:39:02 UTC+08:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text
#+END_COMMENT
----------

#+BEGIN_HTML
<!-- TEASER_END --> 
#+END_HTML
* 安装 TensorFlow
** CPU 版
先开一个虚拟环境
#+BEGIN_SRC bash
conda create --name tensorflow python=2.7
source activate tensorflow 
#+END_SRC

安装必要的库
#+BEGIN_SRC bash
conda install numpy scipy
#+END_SRC

pip 安装 

#+BEGIN_SRC bash
# Ubuntu/Linux 64-bit
$ sudo apt-get install python-pip python-dev
# Ubuntu/Linux 64-bit, CPU only, Python 2.7
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
sudo pip install --upgrade $TF_BINARY_URL
#+END_SRC
*** Demo
** GPU 版
默认是 CUDA 8.0 和CUDNN 4.5 ，不支持 1080
** tensorlayer 把API进行了一些包装
  
#+BEGIN_SRC bash
  pip install git+https://github.com/zsdonghao/tensorlayer.git 
#+END_SRC
* linux

http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu-16-04-nvidia-gtx-1080-cuda-8

http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu16-04-geforce-gtx1080-tensorflow

http://blog.csdn.net/hjimce/article/details/51999566
#+BEGIN_EXAMPLE
cuda8.0+ubuntu16.04+theano、caffe、tensorflow环境搭建

目前自己撘过深度学习各种库、各种环境，已经搭建了n多台电脑，发现每台电脑配置安装方法各不相同，总会出现各不相同的错误，真是心塞。笔记本和台式机有差别，台式机之间的安装方法又各不相同，不同的系统版本环境、平台又各有差异。比如昨天搞的一台电脑，可能因为显卡比较新，然而ubuntu14.04、ubuntu15.04都比较旧，连安装系统都装不上，一开始在14.04上重装了n多次系统，还以为是自己电脑的问题。最后在ubuntu16.04竟然非常顺利完成了安装；然而16.04的版本，只有cuda8.0才支持，在这台破电脑上，又折腾了我快一天的时间。

显卡：GTX960

环境：ubuntu16.04、cuda8.0

下面是我的安装之路，总的来说theano、keras、tensorflow都比较容易安装；最难安装的是caffe，因为caffe调用的第三方库比较杂、比较多。
#+END_EXAMPLE

** 安装 Anaconda
bash Anaconda3-4.1.1-Linux-x86_64.sh
** 安装显卡驱动
会说 X－server 没有关闭
- 切换到单用户模式 CTL+ALT+F1进入
  + 如果要是没有文字，那很有可能是显卡驱动的问题（看，我要装个显卡就这么难）
    1. =sudo apt install bumblebee bumblebee-nvidia primus linux-headers-generic=
    2. 然后重启
- 关闭 lightdm =sudo service lightdm stop=
- 再执行安装，就可以了。
  - sudo sh NVIDIA-Linux-x86_64-367.36.run


- 但是桌面却少了工具栏和任务栏，几经周折，找到了解决方法如下：
  - 就是要重新安装ubuntu desktop：
    - 命令1：
      - =rm -rf ~/.compiz* ~/.config/compiz* ~/.cache/compiz* ~/.gconf/apps/compiz* ~/.config/dconf ~/.cache/dconf ~/.cache/unity=
      - =sudo apt remove ubuntu-desktop=
    - 命令2：
      - =sudo apt install ubuntu-desktop=
    - 命令3：（Ctrl Alt F1 ,然后重启桌面）
      - =sudo service lightdm stop=
      - =sudo service lightdm start=
** 安装 cuda toolbox
- 不要用 cuda 8.0 自带的那个驱动，有的是坑，要用上面的 367.36
- GCC 的版本太高 Ubuntu 16.04 里面是 GCC 5.4 要降级到 4.9
#+BEGIN_SRC bash
  sudo apt-get install g++-4.9
  sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 20
  sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 10
  sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 20
  sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 10
  sudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 30
  sudo update-alternatives --set cc /usr/bin/gcc
  sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 30
  sudo update-alternatives --set c++ /usr/bin/g++
#+END_SRC
- 安装 cuda toolbox
  - sudo sh cuda_8.0.27_linux.run
  - sudo sh cuda_8.0.27.1_linux.run 
- 安装完毕后，再声明一下环境变量，并将其写入到 ~/.bashrc 的尾部:
#+BEGIN_SRC bash
export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
#+END_SRC
*** 测试 
最后再来测试一下CUDA，运行：
**** =nvidia-smi=
**** 再来试几个CUDA例子：
#+BEGIN_SRC bash
  cd 1_Utilities/deviceQuery
  make
#+END_SRC 
 这里如果提示gcc版本过高，可以安装低版本的gcc并做软连接替换，具体方法请自行google，我用低版本的gcc4.9替换了ubuntu16.04自带的gcc5.x版本。

执行 ./deviceQuery ，得到:


#+BEGIN_EXAMPLE

    ./deviceQuery Starting…

    CUDA Device Query (Runtime API) version (CUDART static linking)

    Detected 1 CUDA Capable device(s)

    Device 0: “GeForce GTX 1080”
    CUDA Driver Version / Runtime Version 8.0 / 8.0
    CUDA Capability Major/Minor version number: 6.1
    Total amount of global memory: 8112 MBytes (8506179584 bytes)
    (20) Multiprocessors, (128) CUDA Cores/MP: 2560 CUDA Cores
    GPU Max Clock rate: 1835 MHz (1.84 GHz)
    Memory Clock rate: 5005 Mhz
    Memory Bus Width: 256-bit
    L2 Cache Size: 2097152 bytes
    Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
    Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers
    Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers
    Total amount of constant memory: 65536 bytes
    Total amount of shared memory per block: 49152 bytes
    Total number of registers available per block: 65536
    Warp size: 32
    Maximum number of threads per multiprocessor: 2048
    Maximum number of threads per block: 1024
    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
    Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535)
    Maximum memory pitch: 2147483647 bytes
    Texture alignment: 512 bytes
    Concurrent copy and kernel execution: Yes with 2 copy engine(s)
    Run time limit on kernels: Yes
    Integrated GPU sharing Host Memory: No
    Support host page-locked memory mapping: Yes
    Alignment requirement for Surfaces: Yes
    Device has ECC support: Disabled
    Device supports Unified Addressing (UVA): Yes
    Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0
    Compute Mode:
    < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

    deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 1080
    Result = PASS

#+END_EXAMPLE
** 安装 cuDNN
#+BEGIN_SRC bash
  tar -zxvf cudnn-8.0-linux-x64-v5.0-ga.tgz 
#+END_SRC

    cuda/include/cudnn.h
    cuda/lib64/libcudnn.so
    cuda/lib64/libcudnn.so.5
    cuda/lib64/libcudnn.so.5.0.5
    cuda/lib64/libcudnn_static.a

#+BEGIN_SRC bash
  sudo cp cuda/include/cudnn.h /usr/local/cuda/include/
  sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/
  sudo chmod a+r /usr/local/cuda/include/cudnn.h
  sudo chmod a+r /usr/local/cuda/lib64/libcudnn*
#+END_SRC
** 从源文件安装 TensorFlow
*** Clone the TensorFlow repository

$ git clone https://github.com/tensorflow/tensorflow
*** 安装 Bazel
**** 1、先装jdk
bazel需要Java JDK 8，在ubuntu16.04直接apt-get安装即可：
#+BEGIN_SRC bash
    sudo apt-get update  
    # sudo apt-get install default-jre  
    sudo apt-get install openjdk-8-jre  
    # sudo apt-get install default-jdk  
    sudo apt-get install openjdk-8-jdk  

    sudo update-alternatives --config java
    sudo update-alternatives --config javac
    # 选择8的那个
#+END_SRC
**** 2、安装编译工具Bazel
http://www.bazel.io/docs/install.html#ubuntu

#+BEGIN_SRC bash
  # 从Bazel github上最新的Linux relase版本：
  wget https://github.com/bazelbuild/bazel/releases/download/0.3.0/bazel-0.3.0-installer-linux-x86_64.sh

  # 下载完毕后执行：
  chmod +x bazel-0.3.0-installer-linux-x86_64.sh
  ./bazel-0.3.0-installer-linux-x86_64.sh --user

#+END_SRC


然后在 ~/.bashrc中追加：
#+BEGIN_SRC bash
source /home/zhaoji/.bazel/bin/bazel-complete.bash
export PATH=$PATH:/home/zhaoji/.bazel/bin
#+END_SRC
*** python 的依赖
#+BEGIN_SRC bash
  # For Python 2.7:
  $ sudo apt-get install python-numpy swig python-dev python-wheel
  # For Python 3.x:
  $ sudo apt-get install python3-numpy swig python3-dev python3-wheel 
#+END_SRC
*** 编译安装TensorFlow:
**** 首先从github上克隆TensorFlow最新的代码：
=git clone https://github.com/tensorflow/tensorflow -b v0.10=
**** 代码下载完毕之后，进入tensorflow主目录，执行：
=./configure=
**** Google云平台的支持
Please specify the location of python. [Default is /usr/bin/python]:
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] y
Google Cloud Platform support will be enabled for TensorFlow

    ERROR: It appears that the development version of libcurl is not available. Please install the libcurl3-dev package.

第二项”是否选择Google云平台的支持”选择y之后出现了一个错误，需要libcurl，用apt-get安装，当然，基于国内的网络现状，这一项也可以选择no:
=sudo apt install libcurl3 libcurl3-dev=
**** 安装完毕之后重新执行
=./configure=

除了两处选择yes or no 的地方外，还要注意 compute capability ，其他地方一路回车:

#+BEGIN_EXAMPLE
    Please specify the location of python. [Default is /usr/bin/python]:
    Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] y
    Google Cloud Platform support will be enabled for TensorFlow
    Do you wish to build TensorFlow with GPU support? [y/N] y
    GPU support will be enabled for TensorFlow
    Please specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]:
    Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:
    Please specify the location where CUDA toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
    Please specify the Cudnn version you want to use. [Leave empty to use system default]:
    Please specify the location where cuDNN library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
    Please specify a list of comma-separated Cuda compute capabilities you want to build with.
    You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
    Please note that each additional compute capability significantly increases your build time and binary size.
    [Default is: “3.5,5.2”]:
    Setting up Cuda include
    Setting up Cuda lib64
    Setting up Cuda bin
    Setting up Cuda nvvm
    Setting up CUPTI include
    Setting up CUPTI lib64
    Configuration finished
#+END_EXAMPLE
**** 第一个BUG：修改 CROSSTOOLS file to see this CUDA includes:

You need to update the CROSSTOOLS file to see this CUDA includes:

=tensorflow/third_party/gpus/crosstool/CROSSTOOL=

Around line 65, add:

#+BEGIN_EXAMPLE
cxx_builtin_include_directory: "/usr/local/cuda-8.0/include"
#+END_EXAMPLE

***** 是为了解决这个BUG
#+BEGIN_EXAMPLE
ERROR: /home/zhaoji/TensorFlowDownload/tensorflow/tensorflow/core/kernels/BUILD:1518:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/depthtospace_op_gpu.cu.cc':
  '/usr/local/cuda-8.0/include/cuda_runtime.h'
  '/usr/local/cuda-8.0/include/host_config.h'
  '/usr/local/cuda-8.0/include/builtin_types.h'
  '/usr/local/cuda-8.0/include/device_types.h'
  '/usr/local/cuda-8.0/include/host_defines.h'
  '/usr/local/cuda-8.0/include/driver_types.h'
  '/usr/local/cuda-8.0/include/surface_types.h'
  '/usr/local/cuda-8.0/include/texture_types.h'
  '/usr/local/cuda-8.0/include/vector_types.h'
  '/usr/local/cuda-8.0/include/library_types.h'
  '/usr/local/cuda-8.0/include/channel_descriptor.h'
  '/usr/local/cuda-8.0/include/cuda_runtime_api.h'
  '/usr/local/cuda-8.0/include/cuda_device_runtime_api.h'
  '/usr/local/cuda-8.0/include/driver_functions.h'
  '/usr/local/cuda-8.0/include/vector_functions.h'
  '/usr/local/cuda-8.0/include/vector_functions.hpp'
  '/usr/local/cuda-8.0/include/common_functions.h'
  '/usr/local/cuda-8.0/include/math_functions.h'
  '/usr/local/cuda-8.0/include/math_functions.hpp'
  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.h'
  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.hpp'
  '/usr/local/cuda-8.0/include/cuda_surface_types.h'
  '/usr/local/cuda-8.0/include/cuda_texture_types.h'
  '/usr/local/cuda-8.0/include/device_functions.h'
  '/usr/local/cuda-8.0/include/device_functions.hpp'
  '/usr/local/cuda-8.0/include/device_atomic_functions.h'
  '/usr/local/cuda-8.0/include/device_atomic_functions.hpp'
  '/usr/local/cuda-8.0/include/device_double_functions.h'
  '/usr/local/cuda-8.0/include/device_double_functions.hpp'
  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.h'
  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.hpp'
  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.h'
  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.hpp'
  '/usr/local/cuda-8.0/include/sm_35_atomic_functions.h'
  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.h'
  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.hpp'
  '/usr/local/cuda-8.0/include/sm_20_intrinsics.h'
  '/usr/local/cuda-8.0/include/sm_20_intrinsics.hpp'
  '/usr/local/cuda-8.0/include/sm_30_intrinsics.h'
  '/usr/local/cuda-8.0/include/sm_30_intrinsics.hpp'
  '/usr/local/cuda-8.0/include/sm_32_intrinsics.h'
  '/usr/local/cuda-8.0/include/sm_32_intrinsics.hpp'
  '/usr/local/cuda-8.0/include/sm_35_intrinsics.h'
  '/usr/local/cuda-8.0/include/surface_functions.h'
  '/usr/local/cuda-8.0/include/texture_fetch_functions.h'
  '/usr/local/cuda-8.0/include/texture_indirect_functions.h'
  '/usr/local/cuda-8.0/include/surface_indirect_functions.h'
  '/usr/local/cuda-8.0/include/device_launch_parameters.h'
  '/usr/local/cuda-8.0/include/cuda_fp16.h'
  '/usr/local/cuda-8.0/include/math_constants.h'
  '/usr/local/cuda-8.0/include/curand_kernel.h'
  '/usr/local/cuda-8.0/include/curand.h'
  '/usr/local/cuda-8.0/include/curand_discrete.h'
  '/usr/local/cuda-8.0/include/curand_precalc.h'
  '/usr/local/cuda-8.0/include/curand_mrg32k3a.h'
  '/usr/local/cuda-8.0/include/curand_mtgp32_kernel.h'
  '/usr/local/cuda-8.0/include/cuda.h'
  '/usr/local/cuda-8.0/include/curand_mtgp32.h'
  '/usr/local/cuda-8.0/include/curand_philox4x32_x.h'
  '/usr/local/cuda-8.0/include/curand_globals.h'
  '/usr/local/cuda-8.0/include/curand_uniform.h'
  '/usr/local/cuda-8.0/include/curand_normal.h'
  '/usr/local/cuda-8.0/include/curand_normal_static.h'
  '/usr/local/cuda-8.0/include/curand_lognormal.h'
  '/usr/local/cuda-8.0/include/curand_poisson.h'
  '/usr/local/cuda-8.0/include/curand_discrete2.h'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 294.420s, Critical Path: 248.50s

#+END_EXAMPLE

**** 第二个BUG : zlib not installed

    configure: error: zlib not installed
    Target //tensorflow/cc:tutorials_example_trainer failed to build

google了一下，需要安装zlib1g-dev:
=sudo apt-get install zlib1g-dev=

**** 通过Bazel进行编译安装

#+BEGIN_SRC bash

# To build with GPU support:
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

# The name of the .whl file will depend on your platform.
$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0-py2-none-any.whl
#+END_SRC

需要等待一段时间

编译TensorFlow成功结束的时候，提示如下：

#+BEGIN_EXAMPLE
    Target //tensorflow/cc:tutorials_example_trainer up-to-date:
    bazel-bin/tensorflow/cc/tutorials_example_trainer
    INFO: Elapsed time: 897.845s, Critical Path: 533.72s
#+END_EXAMPLE
**** 执行一下TensorFlow官方文档里的例子，看看能否成功调用GTX 1080：
=bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu=

#+BEGIN_EXAMPLE
    I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
    I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
    I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
    I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
    I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
    I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
    name: GeForce GTX 1080
    major: 6 minor: 1 memoryClockRate (GHz) 1.835
    pciBusID 0000:01:00.0
    Total memory: 7.92GiB
    Free memory: 7.65GiB
    I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
    I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0: Y
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    000003/000006 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    000006/000007 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    000009/000006 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    000009/000004 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    000000/000005 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    000000/000004 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
    ……

#+END_EXAMPLE
没有问题，说明这种通过源代码编译TensorFlow使其支持GPU的方式已经成功了。
**** python 中调用
再在Python中调用一下TensorFlow:
import tensorflow as tf

提示错误：

ImportError: cannot import name pywrap_tensorflow

虽然我们通过源代码安装编译的TensorFlow可用，但是Python版本并没有ready，所以继续：

#+BEGIN_SRC bash
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
sudo pip install /tmp/tensorflow_pkg/tensorflow-0.9.0-py2-none-any.whl
#+END_SRC

=sudo env "PATH=$PATH" pip install bulabula=

    Requirement already satisfied (use –upgrade to upgrade): setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2->tensorflow==0.9.0)
    Installing collected packages: six, funcsigs, pbr, mock, protobuf, tensorflow
    Successfully installed funcsigs-1.0.2 mock-2.0.0 pbr-1.10.0 protobuf-3.0.0b2 six-1.10.0 tensorflow-0.9.0

我们再次打开ipython，试一下tensorflow官方样例:
#+BEGIN_SRC python
Python 2.7.12 (default, Jul  1 2016, 15:12:24)
Type "copyright", "credits" or "license" for more information.

IPython 2.4.1 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:108 successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108 successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108 successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108 successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108 successfully opened CUDA library libcurand.so locally

In [2]: import numpy as np

In [3]: x_data = np.random.rand(100).astype(np.float32)

In [4]: y_data = x_data * 0.1 + 0.3

In [5]: W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

In [6]: b = tf.Variable(tf.zeros([1]))

In [7]: y = W * x_data + b

In [8]: loss = tf.reduce_mean(tf.square(y - y_data))

In [9]: optimizer = tf.train.GradientDescentOptimizer(0.5)

In [10]: train = optimizer.minimize(loss)

In [11]: init = tf.initialize_all_variables()

In [12]: sess = tf.Session()
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.835
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.65GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)

In [13]: sess.run(init)

In [14]: for step in range(201):
   ....:     sess.run(train)
   ....:     if step % 20 == 0:
   ....:         print(step, sess.run(W), sess.run(b))
   ....:        
(0, array([-0.10331395], dtype=float32), array([ 0.62236434], dtype=float32))
(20, array([ 0.03067014], dtype=float32), array([ 0.3403711], dtype=float32))
(40, array([ 0.08353967], dtype=float32), array([ 0.30958495], dtype=float32))
(60, array([ 0.09609199], dtype=float32), array([ 0.30227566], dtype=float32))
(80, array([ 0.09907217], dtype=float32), array([ 0.3005403], dtype=float32))
(100, array([ 0.09977971], dtype=float32), array([ 0.30012828], dtype=float32))
(120, array([ 0.0999477], dtype=float32), array([ 0.30003047], dtype=float32))
(140, array([ 0.0999876], dtype=float32), array([ 0.30000722], dtype=float32))
(160, array([ 0.09999706], dtype=float32), array([ 0.30000171], dtype=float32))
(180, array([ 0.09999929], dtype=float32), array([ 0.30000043], dtype=float32))
(200, array([ 0.09999985], dtype=float32), array([ 0.3000001], dtype=float32))

#+END_SRC

终于OK了，之后就可以尽情享用基于GTX 1080 GPU版的TensorFlow了。

#+BEGIN_SRC python
  import tensorflow as tf

  import numpy as np

  x_data = np.random.rand(100).astype(np.float32)

  y_data = x_data * 0.1 + 0.3

  W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

  b = tf.Variable(tf.zeros([1]))

  y = W * x_data + b

  loss = tf.reduce_mean(tf.square(y - y_data))

  optimizer = tf.train.GradientDescentOptimizer(0.5)

  train = optimizer.minimize(loss)

  init = tf.initialize_all_variables()

  sess = tf.Session()
  sess.run(init)

  for step in range(201):
      sess.run(train)
      if step % 20 == 0:
          print(step, sess.run(W), sess.run(b))

#+END_SRC
* 修补图像 
http://bamos.github.io/2016/08/09/deep-completion/

#+BEGIN_SRC bash
  # 生成一个工作空间
  cd git/
  mkdir tensorflow
  cd tensorflow
  ## git 下来 
  # git clone https://github.com/cmusatyalab/openface.git
  # cd openface
  # pip2 install -r requirements.txt
  # python2 setup.py install
  # cd .. 

  git clone https://github.com/bamos/dcgan-completion.tensorflow.git
#+END_SRC
git 不好用，还是看下面的这个吧[[*OpenFace Docker][OpenFace Docker]] 


Next download a dataset of face images. It doesn’t matter if they have labels or not, we’ll get rid of them. A non-exhaustive list of options are: MS-Celeb-1M, CelebA, CASIA-WebFace, FaceScrub, LFW, and MegaFace. Place the dataset in dcgan-completion.tensorflow/data/your-dataset/raw to indicate it’s the dataset’s raw images.

Now we’ll use OpenFace’s alignment tool to pre-process the images to be 64x64.


#+BEGIN_SRC bash
# -v 参数是加载文件夹用的
sudo docker run -p 9000:9000 -p 8000:8000 -t -v /home/zhaoji/git/tensorflow/dcgan-completion.tensorflow/data/LFW:/LFW -i bamos/openface /bin/bash

/root/openface/util/align-dlib.py /LFW/raw align innerEyesAndBottomLip /LFW/aligned --size 64
#+END_SRC

And finally we’ll flatten the aligned images directory so that it just contains images and no sub-directories.

#+BEGIN_SRC bash
cd LFW/aligned
find . -name '*.png' -exec mv {} . \;
find . -type d -empty -delete
cd ../
#+END_SRC

We’re ready to train the DCGAN. After installing TensorFlow, start the training.

#+BEGIN_SRC bash
./train-dcgan.py --dataset ./data/your-dataset/aligned --epoch 20 
#+END_SRC
You can check what randomly sampled images from the generator look like in the samples directory. I’m training on the CASIA-WebFace and FaceScrub datasets because I had them on hand. After 14 epochs, the samples from mine look like:
** OpenFace Docker
The quickest way to getting started is to use our pre-built automated Docker build, which is available from bamos/openface. This does not require or use a locally checked out copy of OpenFace. To use on your images, share a directory between your host and the Docker container.

#+BEGIN_SRC bash
sudo docker pull bamos/openface
sudo docker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash
cd /root/openface
./demos/compare.py images/examples/{lennon*,clapton*}
./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg
./demos/web/start-servers.sh
#+END_SRC
* 基本知识

使用图 (graph) 来表示计算任务.
在被称之为 会话 (Session) 的上下文 (context) 中执行图.
使用 tensor 表示数据.
通过 变量 (Variable) 维护状态.
使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.
** 变量与运算 Variables and ops
** 两种作用域：变量作用域 variable_scope 与命名作用域 name_scope
*** 变量作用域目标是为了影响变量 variable ，对于其它的运算 op 同样也会影响

with tf.variable_scope("name"), this implicitly opens a tf.name_scope("name").

    #+BEGIN_SRC python
    with tf.variable_scope("foo"):
        x = 1.0 + tf.get_variable("v", [1])
        assert x.op.name == "foo/add"
    #+END_SRC
*** 而 name_scope 不一定改变 Variable 的名字
  - 用 tf.Variable 创建 variable 前面会加上 name_scope 也就是所有的scope
  - 用 tf.get_variable 创建 variable 前面只会加上 variable_scope
  
#+BEGIN_SRC python :results output
  import tensorflow as tf
  with tf.variable_scope("foo"):
      with tf.name_scope("bar"):
          v = tf.get_variable("v", [1])
          v1 = tf.Variable(tf.zeros([1]), name = "v1")
          x = 1.0 + v
  print("v  =",v.name)
  print("v1 =",v1.name)
  print("op =",x.op.name)
  # assert v.name == "foo/v:0"
  # assert x.op.name == "foo/bar/add"
#+END_SRC

#+RESULTS:
: v  = foo/v:0
: v1 = foo/bar/v1:0
: op = foo/bar/add
*** 两者的核心区别： tf.get_variable， 用于共享变量
使用 tf.get_variable 创建变量时，会在 variable_scope 中寻找，reuse 的范围是可以
大于 name_scope 的，当然重名的范围也是这么大

- tf.name_scope just add a prefix to all tensor created in that scope (except the vars created with tf.get_variable)
- tf.variable_scope add a prefix to the variables created with tf.get_variable.

这么设计的好处 
- 一般情况下，你不会想要共享变量，如果想要共享，你要用 tf.get_variable 来创建变
  量，并用 tf.variable_scope 来控制它的作用域，这个作用域可以跨 name_scope
- If it were possible to use tf.name_scope in this case, maybe this would
  decrease the code readability.


重名机制的体现
- 用 tf.get_variable 而不是 tf.Variable 来创建变量时， Tensorflow will start
  checking the names of the vars created with the same method to see if they
  collide. 有重名的会报异常，
- 这个时候改变 tf.name_scope 来避免用 tf.get_variable 创建的变量重名，是没用的，仍然会报异常.
- Only tf.variable_scope context manager will effectively change the name of
  your var in this case.
- Or if you want to reuse the variable you should call
  scope.reuse_variables() before creating the var the second time.
*** 切换方法
- 用字符串是叠加

- 用 object （表现为变量名）是跳转

- When opening a variable scope using a captured object instead of a string, we
  do not alter the current name scope for ops.
* 版本
** Batch Norm 巨慢
解决方法 升级到 0.10 正式版

https://github.com/tensorflow/tensorflow/issues/1502

If I recall correctly, the 0.10 release candidate binary had a performance regression in some of the reduction kernels (on certain GPUs). I believe this is fixed now at HEAD, and will be in the actual release which is due any day now.

Can you try with the nightly build and confirm whether this is still a problem?

0.10.0rc0 has some known performance regressions which should be fixed by the actual release.

安装中的BUG——解决方案，升级Bazel

expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead:     data.

https://github.com/tensorflow/tensorflow/issues/4531

