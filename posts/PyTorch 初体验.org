#+TITLE: PyTorch 初体验
#+DATE: 2017-02-24 14:29:43 
#+TAGS: 
#+CATEGORY: 
#+LINK: 
#+DESCRIPTION: 
#+LAYOUT : post

* What is PyTorch?
It's a Python based scientific computing package targeted at two sets of audiences:

- A replacement for numpy to use the power of GPUs
- a deep learning research platform that provides maximum flexibility and speed

* Tensors
Tensors are similar to numpy's ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.
#+BEGIN_SRC python :results output
from __future__ import print_function
import torch
x = torch.Tensor(5, 3)  # construct a 5x3 matrix, uninitialized
x = torch.rand(5, 3)  # construct a randomly initialized matrix
print(x)
print(x.size())
# NOTE: torch.Size is in fact a tuple, so it supports the same operations
y = torch.rand(5, 3)

# addition: syntax 1
print(x + y)
# addition: syntax 2
print(torch.add(x, y))
# addition: giving an output tensor
result = torch.Tensor(5, 3)
torch.add(x, y, out=result)
print(result)
# addition: in-place
print(y.add_(x)) # adds x to y
# Note: Any operation that mutates a tensor in-place is post-fixed with an _
# For example: x.copy_(y), x.t_(), will change x.

# standard numpy-like indexing with all bells and whistles
print(x[:,1])
#+END_SRC

#+RESULTS:
#+begin_example

 0.0606  0.9371  0.8481
 0.1780  0.1218  0.4467
 0.8567  0.6471  0.4605
 0.8658  0.3407  0.2810
 0.3289  0.8045  0.9089
[torch.FloatTensor of size 5x3]

torch.Size([5, 3])

 0.5406  0.9542  1.1802
 0.4299  0.6906  0.5373
 1.3552  1.5920  1.3288
 0.9886  0.4592  0.3802
 0.7663  1.3225  0.9369
[torch.FloatTensor of size 5x3]


 0.5406  0.9542  1.1802
 0.4299  0.6906  0.5373
 1.3552  1.5920  1.3288
 0.9886  0.4592  0.3802
 0.7663  1.3225  0.9369
[torch.FloatTensor of size 5x3]


 0.5406  0.9542  1.1802
 0.4299  0.6906  0.5373
 1.3552  1.5920  1.3288
 0.9886  0.4592  0.3802
 0.7663  1.3225  0.9369
[torch.FloatTensor of size 5x3]


 0.5406  0.9542  1.1802
 0.4299  0.6906  0.5373
 1.3552  1.5920  1.3288
 0.9886  0.4592  0.3802
 0.7663  1.3225  0.9369
[torch.FloatTensor of size 5x3]


 0.9371
 0.1218
 0.6471
 0.3407
 0.8045
[torch.FloatTensor of size 5]

#+end_example

Read later:
100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc.
http://pytorch.org/docs/torch.html

** Numpy Bridge
 Converting a torch Tensor to a numpy array and vice versa is a breeze.
 The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.

*** Converting torch Tensor to numpy Array
  #+BEGIN_SRC python :results output
  import torch
  a = torch.ones(5)
  print(a)
  b = a.numpy()
  print(b)
  a.add_(1)
  print(a)
  print(b) # see how the numpy array changed in value
  #+END_SRC

  #+RESULTS:
  #+begin_example

   1
   1
   1
   1
   1
  [torch.FloatTensor of size 5]

  [ 1.  1.  1.  1.  1.]

   2
   2
   2
   2
   2
  [torch.FloatTensor of size 5]

  [ 2.  2.  2.  2.  2.]
#+end_example

*** Converting numpy Array to torch Tensor
  #+BEGIN_SRC python :results output
  import torch
  import numpy as np
  a = np.ones(5)
  b = torch.from_numpy(a)
  np.add(a, 1, out=a)
  print(a)
  print(b) # see how changing the np array changed the torch Tensor automatically
  b.add_(5)
  print(a)
  print(b) # see how changing the np array changed the torch Tensor automatically
  #+END_SRC

  #+RESULTS:
  #+begin_example
  [ 2.  2.  2.  2.  2.]

   2
   2
   2
   2
   2
  [torch.DoubleTensor of size 5]

  [ 7.  7.  7.  7.  7.]

   7
   7
   7
   7
   7
  [torch.DoubleTensor of size 5]

#+end_example

  All the Tensors on the CPU except a CharTensor support converting to NumPy and back.
** CUDA 运算地
 #+BEGIN_SRC python :results output
 # let us run this cell only if CUDA is available
 import torch
 x = torch.Tensor(5)
 y = torch.Tensor(7)
 if torch.cuda.is_available():
     x = x.cuda()
     y = y.cuda()
     print(x + y)
 #+END_SRC

 #+RESULTS:
* 自动求导

#+BEGIN_SRC python :results output 
import torch
from torch.autograd import Variable
x = Variable(torch.ones(2, 2), requires_grad = True)
print(x)
y = x + 2
print(y)
print(y.creator)
# y was created as a result of an operation, 
# so it has a creator
z = y * y * 3
print(z)
out = z.mean()
print(out)
# let's backprop now
out.backward()

# out.backward() is equivalent to doing out.backward(torch.Tensor([1.0]))
# print gradients d(out)/dx
print(x.grad)
#+END_SRC

#+RESULTS:
#+begin_example
Variable containing:
 1  1
 1  1
[torch.FloatTensor of size 2x2]

Variable containing:
 3  3
 3  3
[torch.FloatTensor of size 2x2]

<torch.autograd._functions.basic_ops.AddConstant object at 0x7f66793f2e88>
Variable containing:
 27  27
 27  27
[torch.FloatTensor of size 2x2]

Variable containing:
 27
[torch.FloatTensor of size 1]

Variable containing:
 4.5000  4.5000
 4.5000  4.5000
[torch.FloatTensor of size 2x2]

#+end_example

#+BEGIN_SRC python :results output 
import torch
from torch.autograd import Variable
x = torch.randn(3)
x = Variable(x, requires_grad = True)
y = x * 2
while y.data.norm() < 1000:
    y = y * 2
print(y)
gradients = torch.FloatTensor([0.1, 1.0, 0.0001])
y.backward(gradients)
print(x.grad)
#+END_SRC

#+RESULTS:
#+begin_example
Variable containing:
  608.2227
  970.2898
-1593.0037
[torch.FloatTensor of size 3]

Variable containing:
  204.8000
 2048.0000
    0.2048
[torch.FloatTensor of size 3]

#+end_example

* 神经网络

#+BEGIN_SRC python :results output
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  from torch.autograd import Variable

  class Net(nn.Module):
      def __init__(self):
          super(Net, self).__init__()
          self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel
          self.conv2 = nn.Conv2d(6, 16, 5)
          self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b
          self.fc2   = nn.Linear(120, 84)
          self.fc3   = nn.Linear(84, 10)

      def forward(self, x):
          x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window
          x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number
          x = x.view(-1, self.num_flat_features(x))
          x = F.relu(self.fc1(x))
          x = F.relu(self.fc2(x))
          x = self.fc3(x)
          return x
    
      def num_flat_features(self, x):
          size = x.size()[1:] # all dimensions except the batch dimension
          num_features = 1
          for s in size:
              num_features *= s
          return num_features

  net = Net()
  print(net)
  # You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd.
  # You can use any of the Tensor operations in the forward function.
  # The learnable parameters of a model are returned by net.parameters()
  params = list(net.parameters())
  print(len(params))
  print(params[0].size()) # conv1's .weight
  # The input to the forward is an autograd.Variable, and so is the output.

  input = Variable(torch.randn(1, 1, 32, 32))
  out = net(input)
  print(out)
  net.zero_grad() # zeroes the gradient buffers of all parameters
  out.backward(torch.randn(1, 10)) # backprops with random gradients

  # NOTE: torch.nn only supports mini-batches
  # The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.
  # For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.
  # If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.
  # Recap of all the classes you've seen so far:
  ## torch.Tensor - A multi-dimensional array.
  ## autograd.Variable - Wraps a Tensor and records the history of operations applied to it. Has the same API as a  Tensor, with some additions like backward(). Also holds the gradient w.r.t. the tensor.
  ## nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.
  ## nn.Parameter - A kind of Variable, that is automatically registered as a parameter when assigned as an attribute to a Module.
  ## autograd.Function - Implements forward and backward definitions of an autograd operation. Every Variable operation, creates at least a single Function node, that connects to functions that created a Variable and encodes its history.

  # At this point, we covered:
  ## Defining a neural network
  ## Processing inputs and calling backward.
  # Still Left:
  ## Computing the loss
  ## Updating the weights of the network

  # A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.
  # There are several different loss functions under the nn package.
  # A simple loss is: nn.MSELoss which computes the mean-squared error between the input and the target.
  # For example:
  output = net(input)
  target = Variable(torch.range(1, 10))  # a dummy target, for example
  criterion = nn.MSELoss()
  loss = criterion(output, target)
  print(loss)
  # Now, if you follow loss in the backward direction, using it's .creator attribute, you will see a graph of computations that looks like this:
  # input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  
  #       -> view -> linear -> relu -> linear -> relu -> linear 
  #       -> MSELoss
  #       -> loss
  # So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.
  # For illustration, let us follow a few steps backward
  print(loss.creator) # MSELoss
  print(loss.creator.previous_functions[0][0]) # Linear
  print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU

  # now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.
  net.zero_grad() # zeroes the gradient buffers of all parameters
  print('conv1.bias.grad before backward')
  print(net.conv1.bias.grad)
  loss.backward()
  print('conv1.bias.grad after backward')
  print(net.conv1.bias.grad)
  # Now, we have seen how to use loss functions.
  # Read Later:
  # The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here: http://pytorch.org/docs/nn.html
  # The only thing left to learn is:
  # updating the weights of the network

  # The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):
  #  weight = weight - learning_rate * gradient
  # We can implement this using simple python code:
  ### learning_rate = 0.01
  ### for f in net.parameters():
  ###     f.data.sub_(f.grad.data * learning_rate)
  # However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.
  # To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:

  import torch.optim as optim
  # create your optimizer
  optimizer = optim.SGD(net.parameters(), lr = 0.01)

  # in your training loop:
  optimizer.zero_grad() # zero the gradient buffers
  output = net(input)
  loss = criterion(output, target)
  loss.backward()
  optimizer.step() # Does the update
  # This is it.
#+END_SRC

#+RESULTS:
#+begin_example
Net (
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear (400 -> 120)
  (fc2): Linear (120 -> 84)
  (fc3): Linear (84 -> 10)
)
10
torch.Size([6, 1, 5, 5])
Variable containing:
-0.1053 -0.0566 -0.0178 -0.0432  0.0881 -0.0544 -0.0245 -0.1364 -0.0303  0.0169
[torch.FloatTensor of size 1x10]

Variable containing:
 38.8443
[torch.FloatTensor of size 1]

<torch.nn._functions.thnn.auto.MSELoss object at 0x7ff2e9ba6108>
<torch.nn._functions.linear.Linear object at 0x7ff2e9ba6048>
<torch.nn._functions.thnn.auto.Threshold object at 0x7ff2e9ba2f48>
conv1.bias.grad before backward
Variable containing:
 0
 0
 0
 0
 0
 0
[torch.FloatTensor of size 6]

conv1.bias.grad after backward
Variable containing:
1.00000e-02 *
  4.0396
 -0.0561
 -1.9786
 -6.0124
 -2.6301
  0.8892
[torch.FloatTensor of size 6]

#+end_example
